{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d294f676",
   "metadata": {},
   "source": [
    "### Problem 1: basic operations in PyTorch\n",
    "- 1.1 Create a 3x3 tensor filled with zeros and another one filled with ones. Perform element-wise addition and subtraction between the two tensors.\n",
    "\n",
    "- 1.2 Create a tensor of shape (3, 4) where each element is taken from a uniform distribution in the interval $[-1, 2]$. Calculate its mean and standard deviation using PyTorch functions.\n",
    "\n",
    "- 1.3 Create two tensors of different shapes (e.g., one (3, 4) and another (4, 3)) and perform matrix multiplication between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16beb98",
   "metadata": {},
   "source": [
    "### 1.1 Create a 3x3 tensor filled with zeros and another one filled with ones. Perform element-wise addition and subtraction between the two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e5a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise Addition: \n",
      "  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Element-wise Subtraction: \n",
      "  tensor([[-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = torch.zeros([3, 3])\n",
    "b = torch.ones([3, 3])\n",
    "\n",
    "print('Element-wise Addition: \\n ', torch.add(a, b))\n",
    "print('Element-wise Subtraction: \\n ', torch.sub(a, b), '\\n', torch.sub(b, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6272e",
   "metadata": {},
   "source": [
    "### 1.2 Create a tensor of shape (3, 4) where each element is taken from a uniform distribution in the interval  [âˆ’1,2]. Calculate its mean and standard deviation using PyTorch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da767894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5193,  0.7880, -0.3425,  1.5424],\n",
      "        [ 1.4085, -0.0309,  1.6380,  0.8759],\n",
      "        [ 1.2010,  1.3906,  1.6726,  0.4333]])\n",
      "Mean of tensor c =  tensor(0.9247) \n",
      " Standard deviation of tensor c =  tensor(0.6690)\n"
     ]
    }
   ],
   "source": [
    "c = torch.distributions.uniform.Uniform(-1, 2).sample([3, 4])\n",
    "print(c)\n",
    "print('Mean of tensor c = ', torch.mean(c), '\\n', 'Standard deviation of tensor c = ', torch.std(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76cf227",
   "metadata": {},
   "source": [
    "### 1.3 Create two tensors of different shapes (e.g., one (3, 4) and another (4, 3)) and perform matrix multiplication between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f382b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication: \n",
      " tensor([[[[39, 12, 26, 49],\n",
      "          [36, 20, 48, 56]]]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.tensor([[1, 3, 2, 4], [3, 1, 5, 5]])\n",
    "e = torch.tensor(([[[[4, 3, 2, 1], [9, 1, 2, 8], [2, 1, 7, 6], [1, 1, 1, 3]]]]))\n",
    "print('Matrix multiplication: \\n', torch.matmul(d, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d22568",
   "metadata": {},
   "source": [
    "### Problem 2: broadcasting in PyTorch\n",
    "First read the broadcasting tutorial for Numpy and PyTorch:\n",
    "- Numpy: https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "- PyTorch: https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "\n",
    "Then finish the following problem:\n",
    "- 2.1 Create a tensor of shape (3, 4) and add a 1D tensor (e.g., shape (4,)) to it. Explain the result using broadcasting.\n",
    "- 2.2 Multiply a tensor of shape (5, 1) by a 1D tensor (e.g., shape (5,)). Describe how broadcasting affects the multiplication.\n",
    "- 2.3 Create a tensor of shape (3, 4) and add a 2D tensor (e.g., shape (3, 1)) to it. Explain how broadcasting expands the dimensions for addition.\n",
    "- 2.4 Multiply a 3D tensor of shape (4, 1, 3) by a 2D tensor of shape (1, 3). Describe the result and how broadcasting operates in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a160e",
   "metadata": {},
   "source": [
    "### 2.1 Create a tensor of shape (3, 4) and add a 1D tensor (e.g., shape (4,)) to it. Explain the result using broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09e2e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[6, 4, 4, 4],\n",
      "         [4, 2, 9, 9],\n",
      "         [3, 2, 3, 6]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(([[[4, 3, 2, 1], [2, 1, 7, 6], [1, 1, 1, 3]]]))\n",
    "t2 = torch.tensor([2, 1, 2, 3])\n",
    "print(t1 + t2) # or torch.add(t1, t2)\n",
    "\n",
    "# The result is a (3, 4) tensor.\n",
    "# The one dimensional tensor is stretched to match the size of the (3, 4) tensor by replicating into 3 rows so that the \n",
    "# element of each row of t2 is broadcasted to the element in the same position in t1 for addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ace45",
   "metadata": {},
   "source": [
    "### 2.2 Multiply a tensor of shape (5, 1) by a 1D tensor (e.g., shape (5,)). Describe how broadcasting affects the multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c1667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "y =  tensor([-0.0335, -0.3700,  0.1075, -0.4375,  1.0039])\n",
      "x * y =  tensor([[-0.0335, -0.3700,  0.1075, -0.4375,  1.0039],\n",
      "        [-0.0335, -0.3700,  0.1075, -0.4375,  1.0039],\n",
      "        [-0.0335, -0.3700,  0.1075, -0.4375,  1.0039],\n",
      "        [-0.0335, -0.3700,  0.1075, -0.4375,  1.0039],\n",
      "        [-0.0335, -0.3700,  0.1075, -0.4375,  1.0039]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 1)\n",
    "y = torch.randn(5, )\n",
    "\n",
    "print('x = ', x)\n",
    "print('y = ', y)\n",
    "print('x * y = ', torch.mul(x, y)) # or (x * y)\n",
    "\n",
    "# Here, our  result is a (5, 5) tensor.\n",
    "# Pytorch identifies a backward compatibility because the tensors here which are broadcastable have different shapes \n",
    "# but the same number of elements. Each tensor stretched to match the number of rows/columns in the other tensor. Then \n",
    "# element-wise multiplication is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2733b37",
   "metadata": {},
   "source": [
    "### 2.3 Create a tensor of shape (3, 4) and add a 2D tensor (e.g., shape (3, 1)) to it. Explain how broadcasting expands the dimensions for addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "122f2563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 =  tensor([[1, 3, 1, 1],\n",
      "        [2, 1, 1, 1],\n",
      "        [3, 2, 3, 2]])\n",
      "t4 =  tensor([[4],\n",
      "        [5],\n",
      "        [5]])\n",
      "t3 + t4 =  tensor([[5, 7, 5, 5],\n",
      "        [7, 6, 6, 6],\n",
      "        [8, 7, 8, 7]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.randint(1, 4, (3, 4))\n",
    "t4 = torch.randint(3, 6, (3, 1))\n",
    "\n",
    "print('t3 = ', t3)\n",
    "print('t4 = ', t4)\n",
    "print('t3 + t4 = ', torch.add(t3, t4))\n",
    "\n",
    "# The result is a (3, 4) tensor. These tenspors are braodcastable for addition because they have the same number of rows.\n",
    "# Like 3.1 above, the 2D tensor is expanded in dimension column-wise to match the size of the (3, 4) tensor by replicating \n",
    "# into 4 columns so that the element of each row of t3 is broadcasted to the element in the same position in t4 for \n",
    "# element-wise addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4af454",
   "metadata": {},
   "source": [
    "### 2.4 Multiply a 3D tensor of shape (4, 1, 3) by a 2D tensor of shape (1, 3). Describe the result and how broadcasting operates in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d595db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5 =  tensor([[[3, 3, 3]],\n",
      "\n",
      "        [[1, 1, 2]],\n",
      "\n",
      "        [[2, 3, 1]],\n",
      "\n",
      "        [[1, 2, 1]]])\n",
      "t6 =  tensor([[4, 3, 3]])\n",
      "t5 * t6 =  tensor([[[12,  9,  9]],\n",
      "\n",
      "        [[ 4,  3,  6]],\n",
      "\n",
      "        [[ 8,  9,  3]],\n",
      "\n",
      "        [[ 4,  6,  3]]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.randint(1, 4, (4, 1, 3))\n",
    "t6 = torch.randint(3, 6, (1, 3))\n",
    "\n",
    "print('t5 = ', t5)\n",
    "print('t6 = ', t6)\n",
    "print('t5 * t6 = ', torch.mul(t5, t6))\n",
    "\n",
    "# The result is a 3D tensor of shape (4, 1, 3). First the two tensors are broadcastable because in the 1nd and 2rd trailing \n",
    "# dimensions, x size == y size.\n",
    "# So broadcasting creates a first trailing dimension of same size as t5 for t6 so that t5 size = t6 size, and it does \n",
    "# element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05198f25",
   "metadata": {},
   "source": [
    "### Problem 3:\n",
    "The following code is taken from the lecture note week3-gradient-descent. It implements the gradient descent method for the linear regression problem.\n",
    "\n",
    "Replace the part on the gradient descent method (i.e. the function gradient(X, y, w)) to mini-batch stochastic gradient descent method with different mini-batch size 16, 32, 64. Show the result (including the result w and the history of the loss during iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8e343d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , Loss:  0.012254493553822854 , Slope:  3.0802101495897873 , Intercept:  3.8959833219915425\n",
      "Epoch:  100 , Loss:  0.009388981540211436 , Slope:  2.9896950642558955 , Intercept:  3.998711776732756\n",
      "Epoch:  200 , Loss:  0.009377810474703223 , Slope:  2.9994885384892696 , Intercept:  4.0122366394333735\n",
      "Epoch:  300 , Loss:  0.009251533476782375 , Slope:  2.9912436779647678 , Intercept:  4.006211334784177\n",
      "Epoch:  400 , Loss:  0.009584644074174826 , Slope:  3.0005735263386826 , Intercept:  4.0180502488409235\n",
      "Epoch:  500 , Loss:  0.009246480018580732 , Slope:  2.9917089529221004 , Intercept:  4.006630066649127\n",
      "Epoch:  600 , Loss:  0.009420606839088802 , Slope:  3.0030083429304875 , Intercept:  4.009867711244844\n",
      "Epoch:  700 , Loss:  0.009414944940874626 , Slope:  2.986453981350423 , Intercept:  4.001344761660903\n",
      "Epoch:  800 , Loss:  0.009325816804288253 , Slope:  2.9960726402354654 , Intercept:  3.995212721613862\n",
      "Epoch:  900 , Loss:  0.009246366775358974 , Slope:  2.9976719218922354 , Intercept:  4.001801922993416\n",
      "\n",
      "Batch size:  16\n",
      "End loss:  0.009856396858156289\n",
      "Slope:  [2.98191827]\n",
      "Intercept:  [3.99446544]\n",
      "\n",
      "Epoch:  0 , Loss:  0.0326101635429781 , Slope:  3.248106029804813 , Intercept:  3.702868916447566\n",
      "Epoch:  100 , Loss:  0.00925657059341601 , Slope:  2.9974277356824235 , Intercept:  4.006428746335648\n",
      "Epoch:  200 , Loss:  0.009244955563502827 , Slope:  2.9932261987279714 , Intercept:  4.005168878746301\n",
      "Epoch:  300 , Loss:  0.009456512722071361 , Slope:  3.0020569587569588 , Intercept:  4.012372093395671\n",
      "Epoch:  400 , Loss:  0.009324080055261212 , Slope:  2.9892453138973387 , Intercept:  4.0023563068560986\n",
      "Epoch:  500 , Loss:  0.009242626564081941 , Slope:  2.9948393234029256 , Intercept:  4.004348732179057\n",
      "Epoch:  600 , Loss:  0.009298413308304127 , Slope:  2.9916418763878703 , Intercept:  4.001245691948068\n",
      "Epoch:  700 , Loss:  0.009256382417291573 , Slope:  2.9982530683158313 , Intercept:  4.005262771268809\n",
      "Epoch:  800 , Loss:  0.009256973600893486 , Slope:  2.9932151741585256 , Intercept:  4.003173818182737\n",
      "Epoch:  900 , Loss:  0.00930850335665701 , Slope:  2.9984860684947385 , Intercept:  4.009764274755873\n",
      "\n",
      "Batch size:  32\n",
      "End loss:  0.009302800490008239\n",
      "Slope:  [2.99026839]\n",
      "Intercept:  [4.00248251]\n",
      "\n",
      "Epoch:  0 , Loss:  0.06692812355962398 , Slope:  3.4090352964814317 , Intercept:  3.544406623129936\n",
      "Epoch:  100 , Loss:  0.009262151327723563 , Slope:  2.9964174970596797 , Intercept:  4.008393112719204\n",
      "Epoch:  200 , Loss:  0.009270683907943788 , Slope:  2.9913206737439695 , Intercept:  4.003777104626081\n",
      "Epoch:  300 , Loss:  0.00932219793754054 , Slope:  2.9984573882790673 , Intercept:  4.01063054913105\n",
      "Epoch:  400 , Loss:  0.009240904449369182 , Slope:  2.9941030287541173 , Intercept:  4.006103034531513\n",
      "Epoch:  500 , Loss:  0.009242124974620063 , Slope:  2.993697114702312 , Intercept:  4.005571663751884\n",
      "Epoch:  600 , Loss:  0.00924088640371458 , Slope:  2.9935618438443328 , Intercept:  4.00686351667963\n",
      "Epoch:  700 , Loss:  0.009368813498107278 , Slope:  2.987855896384771 , Intercept:  4.001680849431502\n",
      "Epoch:  800 , Loss:  0.00926111666582118 , Slope:  2.991506341482324 , Intercept:  4.004563385297473\n",
      "Epoch:  900 , Loss:  0.009241076527076966 , Slope:  2.993468094321817 , Intercept:  4.006509663290824\n",
      "\n",
      "Batch size:  64\n",
      "End loss:  0.00924831637633268\n",
      "Slope:  [2.99268984]\n",
      "Intercept:  [4.00504276]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate some random data\n",
    "N = 1024\n",
    "X = 2 * np.random.rand(N, 1)\n",
    "y = 4 + 3 * X + np.random.randn(N, 1) * 0.1\n",
    "\n",
    "# Add a column of ones to the feature matrix for the bias term\n",
    "X_b = np.hstack((X, np.ones((N, 1))))\n",
    "\n",
    "# Initialize the weights to zeros\n",
    "w = np.zeros((2, 1))\n",
    "\n",
    "# Set the learning rate and number of iterations\n",
    "alpha = 0.1\n",
    "num_iters = 1000\n",
    "\n",
    "# Define the gradient of the cost function\n",
    "def gradient(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    grad = (2 / m) * X.T.dot(X.dot(w) - y)\n",
    "    return grad\n",
    "\n",
    "# Define the MSE (loss) of the cost function\n",
    "def loss_comp(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    loss = np.mean((X.dot(w) - y) ** 2)\n",
    "    return loss\n",
    "    \n",
    "# Define batch sizes loss history\n",
    "batch_sizes = [16, 32, 64]\n",
    "loss_history = {16: [], 32: [], 64: []}\n",
    "    \n",
    "# Run mini-batch stuchastic gradient descent to update the weights with the different batch sizes\n",
    "for bs in batch_sizes:\n",
    "    w = np.zeros((2, 1))\n",
    "    for i in range(num_iters):\n",
    "        grad = gradient(X_b, y, w)\n",
    "        rearr = np.random.permutation(N)\n",
    "        X_rearr = X_b[rearr]\n",
    "        y_rearr = y[rearr]\n",
    "        for j in range(0, N, bs):\n",
    "            X_batch = X_rearr[j:j + bs]\n",
    "            y_batch = y_rearr[j:j + bs]\n",
    "            grad = gradient(X_batch, y_batch, w)\n",
    "            w = w - alpha * grad\n",
    "        # Compute and store loss\n",
    "        loss = loss_comp(X_b, y, w)\n",
    "        loss_history[bs].append(loss)\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch:', i, ',', 'Loss:', loss, ',', 'Slope:', w[0][0], ',', 'Intercept:', w[1][0])\n",
    "    # Print the learned parameters\n",
    "    print()\n",
    "    print(\"Batch size: \", bs)\n",
    "    print(\"End loss: \", loss_history[bs][-1])\n",
    "    print(\"Slope: \", w[0])\n",
    "    print(\"Intercept: \", w[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18df70",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. How difficult do you think of the homework? Please give your marks between 1-5 (1 is the easiest and 5 is the most difficult).\n",
    "\n",
    "Ans: 3\n",
    "\n",
    "2. How difficult do you think of the lecture? Please give your marks between 1-5 (1 is the easiest and 5 is the most difficult).\n",
    "\n",
    "Ans: 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
